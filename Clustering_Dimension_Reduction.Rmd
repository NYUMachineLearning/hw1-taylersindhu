---
title: "Homework 1- Machine Learning"
author: "Tayler Sindhu"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

## Homework
```{r include=FALSE}
# Loading libraries
library(ggplot2)
library(fastICA)
library(tidyverse)
library(ggfortify)
library(cluster)
```

```{r include=FALSE}
data(iris)
#You can choose however many clusters you want
head(iris)
```

0. Subset the Iris dataset to only include `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. 
```{r}
# Dropped "species" column
iris_subset <- select(iris, -"Species")
head(iris_subset, n = 3)
```

**1. Write out the Kmeans algorithm by hand, and run two iterations of it.**
```{r}
# Use set.seed function so data is replicable
set.seed(4)

# K = 3
# Randomly assign each data point to a number from 1-3
iris_subset$Cluster <- sample(1:3, nrow(iris), replace = TRUE)

# First Iteration

# Compute cluster centroid
# There are 4 variables, so the centroids will be vectors of length 4
cluster_1 <- filter(iris_subset, Cluster == 1)
cluster_2 <- filter(iris_subset, Cluster == 2)
cluster_3 <- filter(iris_subset, Cluster == 3)

a <- as.vector(apply(cluster_1, 2, mean)[1:4])
b <- as.vector(apply(cluster_2, 2, mean)[1:4])
c <- as.vector(apply(cluster_3, 2, mean)[1:4])

# Euclidean distance of each point from each centroid
d <- sqrt(((iris_subset[1]-a[1])^2)+((iris_subset[2]-a[2])^2)+((iris_subset[3]-a[3])^2)+((iris_subset[4]-a[4])^2))
e <- sqrt(((iris_subset[1]-b[1])^2)+((iris_subset[2]-b[2])^2)+((iris_subset[3]-b[3])^2)+((iris_subset[4]-b[4])^2))
f <- sqrt(((iris_subset[1]-c[1])^2)+((iris_subset[2]-c[2])^2)+((iris_subset[3]-c[3])^2)+((iris_subset[4]-c[4])^2))

df <- data.frame(d,e,f)

# Finds minimum distance
iris_subset$Min_dist_1 <- apply(df, 1, min)

# Assigns to new cluster
iris_subset$Cluster_2 <- apply(df, 1, which.min)

# Second Iteration

# Computer cluster centroid
cluster_1.1 <- filter(iris_subset, Cluster_2 == 1)
cluster_2.1 <- filter(iris_subset, Cluster_2 == 2)
cluster_3.1 <- filter(iris_subset, Cluster_2 == 3)

g <- as.vector(apply(cluster_1.1, 2, mean)[1:4])
h <- as.vector(apply(cluster_2.1, 2, mean)[1:4])
i <- as.vector(apply(cluster_3.1, 2, mean)[1:4])

j <- sqrt(((iris_subset[1] - g[1])^2)+((iris_subset[2] - g[2])^2)+((iris_subset[3] - g[3])^2)+((iris_subset[4] - g[4])^2))
k <- sqrt(((iris_subset[1] - h[1])^2)+((iris_subset[2] - h[2])^2)+((iris_subset[3] - h[3])^2)+((iris_subset[4] - h[4])^2))
l <- sqrt(((iris_subset[1] - i[1])^2)+((iris_subset[2] - i[2])^2)+((iris_subset[3] - i[3])^2)+((iris_subset[4] - i[4])^2))

df1 <- data.frame(j, k, l)

# Finds Minimum Distance
iris_subset$Min_dist_2 <- apply(df1, 1, min)

# Assigns to new cluster
iris_subset$Cluster_3 <- apply(df1, 1, which.min)

# Displays clusters in each iteration
iris_clusters <- select(iris_subset, -c("Min_dist_1","Min_dist_2"))
iris_clusters                     
```
```{r echo=TRUE}
# Plotting first two variables to get an idea of clusters, although would have used principal component analysis if prompted

plot(iris_subset$Sepal.Length, iris_subset$Sepal.Width, col=iris_subset$Cluster, main = "K-Means Clustering with k=3: Initial Cluster Assignment", xlab = "Sepal Length", ylab = "Sepal Width")
plot(iris_subset$Sepal.Length, iris_subset$Sepal.Width, col=iris_subset$Cluster_2, main = "K-Means Clustering with k=3: First Iteration", xlab = "Sepal Length", ylab = "Sepal Width")
plot(iris_subset$Sepal.Length, iris_subset$Sepal.Width, col=iris_subset$Cluster_3, main = "K-Means Clustering with k=3: Second Iteration", xlab = "Sepal Length", ylab = "Sepal Width")
```

**2. Run PCA on the Iris dataset. Plot a scatter plot of PC1 vs PC2 and include the percent variance those PCs describe.**
```{r}
# Converting data into matrix 
iris_pca <- data.matrix(iris[,1:4])

# Running PCA
pr_iris <- prcomp(iris_pca, scale.=TRUE)
pr_iris

# Plotting 
autoplot(prcomp(iris_pca, scale.=TRUE))

# Calculating percent variance explained by each principle component to check plot labels

# Variance of each principle component
pr_iris$sdev^2

# Sum of variance of each principle component
sum(pr_iris$sdev^2)

# Percent variance each principle component describes.
per_var <- pr_iris$sdev^2/sum(pr_iris$sdev^2)

# Isolating PC1 and PC2, which cumulatively describe 95.81% of the variance. PC1 describes 72.96% of the variance, and PC2 describes 22.85% of the variance. 
per_var[1:2]
```

**3. Run ICA on the Iris dataset. Plot the independent components as a heatmap.**
```{r}
iris_ica <- data.matrix(iris[,1:4])
ica_output <- fastICA(iris_ica, 4, fun = "logcosh", alpha = 1, row.norm = FALSE, maxit = 200, tol = 0.0001, verbose = FALSE)
ica_output
```
```{r}
# Plotting as heatmap
heatmap(ica_output$S)
```

**4. Use Kmeans to cluster the Iris data.**
  * Use the silhouette function in the cluster package to find the optimal number of clusters for kmeans for the iris dataset. Then cluster using kmeans clustering. Does the data cluster by species? 
  * Using this clustering, color the PCA plot according to the clusters.**
```{r}
# Running k-means
set.seed(4)

# Trying with two clusters
kmeans_iris_2 <- kmeans(iris[,1:4], 2)

a <- silhouette(kmeans_iris_2$cluster, dist(iris[,1:4]))

plot(a, col=iris$Species)

# Trying with three clusters
kmeans_iris_3 <- kmeans(iris[,1:4], 3)

b <- silhouette(kmeans_iris_3$cluster, dist(iris[,1:4]))

plot(b, col=iris$Species)

# Trying with four clusters
kmeans_iris_4 <- kmeans(iris[,1:4], 4)

c <- silhouette(kmeans_iris_4$cluster, dist(iris[,1:4]))

plot(c, col=iris$Species)

# Checking which species are in which cluster
levels(iris$Species)
palette()

# Per silhouette analysis, two clusters results in the highest average silhouette width. The data cluster somewhat by species, in that one species (setosa) is in the first cluster, and the vast majority of the remaining two species (versicolor and virginica) are in the second cluster; however, the clusters do not each represent a species.

# Running K means with two clusters
kmeans_iris_2 <- kmeans(iris[,1:4], 2)
kmeans_iris_2

# PCA plot colored by clusters
autoplot(prcomp(iris_pca, scale.=TRUE), data = iris, col = kmeans_iris_2$cluster)
```
  
**5. Use hierarchical clustering to cluster the Iris data.**

  * Try two different linkage types, and two different distance metrics. 
  * For one linkage type and one distance metric, try two different cut points. 
  * Using this clustering, color the PCA plot according to the clusters. (6  plots in total)
```{r warning=FALSE}
# First linkage type and distance metric
iris_dist <- dist(iris[,1:4], method = "manhattan")
iris_dend <- hclust(iris_dist, method = "average")
plot(iris_dend)

# Second linkage type and distance metric
iris_dist_2 <- dist(iris[,1:4], method = "euclidean")
iris_dend_2 <- hclust(iris_dist_2, method = "complete")
plot(iris_dend_2)
```
```{r}
# Trying two different cut points
plot(iris_dend_2)
rect.hclust(iris_dend_2, k = 3)

plot(iris_dend_2)
rect.hclust(iris_dend_2, k = 2)
```

```{r}
# Coloring PCA plot according to clustering
tree_2 <- cutree(iris_dend_2, k = 2)
autoplot(prcomp(iris_pca, scale.=TRUE), data = iris, col = tree_2)

tree <- cutree(iris_dend_2, k = 3)
autoplot(prcomp(iris_pca, scale.=TRUE), data = iris, col = tree)
```
  



